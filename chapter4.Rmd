# Clustering and classification / week 4

This week were using a dataset called 'Boston' from the MASS R package. 'Boston' Has the housing values of Boston suburbs, including 14 variables that describe the value of the apartments and the socioeconomical status of the area, such as rate of criminality, per capita crime rate by tow, taxes and pupil-teacher rate per town. There are 506 rows in the dataset. The values are numerical, some of them being integers. More detailed description of the varibales can be found in the [R documentation](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html). 

## Boston dataset summary

```{r access libraries, load the Boston dataset and explore, echo=FALSE, fig.width = 12, fig.height = 12}
# access the MASS package
library(MASS)
library(ggplot2)
library(GGally)
# load the data
data("Boston")

# explore dataset
str(Boston)
summary(Boston)

pairs(Boston, main= 'Boston paired scatterplots')
```



## Scaled data

```{r scale data and sumamrize variables, echo=FALSE}
# scale the data by substracting column means from the values and by dividing with the standard deviation

boston_scaled_orig <- scale(Boston)
summary(boston_scaled_orig)
```


We can see that after scaling, the mean is 0 for each variable, and that variables below the mean have negative values and variables above mean have positive values. The scaling of the data makes the different variables comparable.

Next we want to make a model that predicts the criminality in certain areas based on the other variables. For this purpose we first replace the 'crime' variable with a factorized variable, that we want to predict eventually. The break points for the crime variable are set to the quantiles and the classes are named as 'high', 'med_high', 'med_low' and 'low'.

We also need to divide the data into a training and test sets, so that we can use part of the data to train our model, and then test the validity of the model with the test data. 80% of the data is randomly assigned to training set and the rest of the data is used for testing.

```{r replace crim with factorized values, echo=FALSE, results='hide'}
# summary of the scaled crime rate
boston_scaled <- data.frame(boston_scaled_orig)
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
labels = c("low", "med_low", "med_high", "high")
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=labels)

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

```{r create trainign and test data sets, echo=FALSE}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set (80% od the data)
train <- boston_scaled[ind,]

# create test set (20% of the data)
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

## Linear discriminant analysis and LDA biplot

```{r plot LDA biplot, echo=FALSE, fig.width = 12, fig.height = 12}

# linear discriminant analysis
lda.fit <- lda(crime~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, main='LDA biplot')
lda.arrows(lda.fit, myscale = 1)
```

## Using the model to predict criminality classes in the test dataset
```{r predict the crime classes in the test dataset, echo=FALSE}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the actual and predicted classes
table(correct = correct_classes, predicted = lda.pred$class)
```

From the plot above we can see that areas with high criminality rate form their own cluster, and other classes are closer to each other. The main variable that sets high criminality areas apart from the others is 'rad'(index of accessibility to radial highways). The suburbs that have good access to radial high ways seem to have high criminality rate. 

From the cross tabulation we can see that the model predicts high criminality very well (as can also be predicted from the plots; high criminality forms its own cluster). Also med_low was predicted quite well (only 3 false positives). However, if the model classifies an observation as med_low, it is very hard to know whether the prediction is correct, as more than half of these predictions are false and the model seems to have a bias towards classifying observations into the med_low class. 


## K-means clustering
```{r k-means clustering, echo=FALSE, fig.width = 12, fig.height = 12}
# calculate the euclidean distances from the original scaled Boston dataset
dist_eu <- dist(boston_scaled_orig)

# k-means clustering with 3 centers
km <-kmeans(boston_scaled_orig, centers = 4)

#set seed to ensure that the results are reproducable (randon number generating starts from the seed)
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled_orig, k)$tot.withinss})

# visualize the results.
qplot(x = 1:k_max, y = twcss, geom = 'line', main='total within sum of squares')


# k-means clustering
km <-kmeans(boston_scaled_orig, centers = 2)


# plot the Boston dataset with clusters
pairs(boston_scaled_orig, col = km$cluster, main= 'Boston data (2 clusters
)')
pairs(boston_scaled_orig[,5:10], col = km$cluster, main= 'subset of Boston data (2 clusters
)')

```


Here we did K-means clustering based on euclidean distances. The optimal number of clusters was chosen by finding the point where the total within sum of squares had a great drop. Based on this method, the optimal number of clusters is 2. From the graphs we can see that these two clusters deviate from each other mainly in the 'rad' and 'tax' variables, but the clusters can also be seen in other variables where the differences are more continuous (such as age, rm and nox). 